{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Loading fiqa-pl corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    corpus: Dataset({\n",
       "        features: ['_id', 'title', 'text'],\n",
       "        num_rows: 57638\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'title', 'text'],\n",
       "    num_rows: 57638\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ds['corpus']\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Preparing elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a link for a local elasticsearch, beacuase i disabled ssl in elastic config we do not need to link to certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'http://localhost:9200/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch(['http://localhost:9200'])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(link)\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the index \n",
    "- First we need analyzer one with synonym filter and one without. Both have to have lowercase and morfologik_stem filter\n",
    "- Secondly filter that defines the synonyms for Polish month `kwiecień`\n",
    "- Thirdly mappings one for each anylzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_config = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"polish_with_synonyms\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"synonym_filter\",\n",
    "                        \"morfologik_stem\"\n",
    "                    ]\n",
    "                },\n",
    "                \"polish_without_synonyms\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"morfologik_stem\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"synonym_filter\": {\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": [\"kwiecień, kwi, IV\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content_with_synonyms\": {\"type\": \"text\", \"analyzer\": \"polish_with_synonyms\"},\n",
    "            \"content_without_synonyms\": {\"type\": \"text\", \"analyzer\": \"polish_without_synonyms\"}\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare texts/documents for elastic indexing. We loop through corpus and index every text. Thanks to `helpers.bulk()` function we can efficiently index multiple documents in a single API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import helpers\n",
    "\n",
    "index_name =  \"biore_sie_do_roboty\"\n",
    "\n",
    "def generate_docs(ds):\n",
    "    for doc in ds:\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": doc[\"_id\"],\n",
    "            \"_source\": {\n",
    "                \"content_with_synonyms\": doc[\"text\"], \n",
    "                \"content_without_synonyms\": doc[\"text\"]\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also delete and add the index so that we always create a new on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 57638 documents\n"
     ]
    }
   ],
   "source": [
    "es.options(ignore_status=[400, 404]).indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, body=index_config)\n",
    "\n",
    "try:\n",
    "    success, errors = helpers.bulk(es, generate_docs(corpus))\n",
    "    \n",
    "    print(f\"Successfully indexed {success} documents\")\n",
    "    if errors:\n",
    "        print(f\"Errors during indexing: {errors}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during bulk indexing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the queries for `kwiecień` and see how many times it occures in corpus with and without synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of occurrences of the word 'kwiecień' including its synonyms: 306\n",
      "The number of occurrences of the word 'kwiecień' without synonyms: 257\n"
     ]
    }
   ],
   "source": [
    "query_with_synonyms = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content_with_synonyms\": \"kwiecień\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "query_without_synonyms = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content_without_synonyms\": \"kwiecień\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=index_name, body=query_with_synonyms)\n",
    "print(\"The number of occurrences of the word 'kwiecień' including its synonyms:\",response['hits']['total']['value'])\n",
    "response = es.search(index=index_name, body=query_without_synonyms)\n",
    "print(\"The number of occurrences of the word 'kwiecień' without synonyms:\",response['hits']['total']['value'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Using fiqa-pl-qrels dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Preparing the qa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 14166\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 1238\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 1706\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2 = load_dataset(\"clarin-knext/fiqa-pl-qrels\")\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'title', 'text'],\n",
       "    num_rows: 6648\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds3 = load_dataset(\"clarin-knext/fiqa-pl\", \"queries\")\n",
    "queries = ds3['queries']\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '3',\n",
       " 'title': '',\n",
       " 'text': 'Nie mówię, że nie podoba mi się też pomysł szkolenia w miejscu pracy, ale nie możesz oczekiwać, że firma to zrobi. Szkolenie pracowników to nie ich praca – oni tworzą oprogramowanie. Być może systemy edukacyjne w Stanach Zjednoczonych (lub ich studenci) powinny trochę martwić się o zdobycie umiejętności rynkowych w zamian za ich ogromne inwestycje w edukację, zamiast wychodzić z tysiącami zadłużonych studentów i narzekać, że nie są do niczego wykwalifikowani.'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[566392, 65404]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "corpus_list = []\n",
    "queries_list = []\n",
    "query_to_corpus = defaultdict(list)\n",
    "\n",
    "for query_id, corpus_id in zip(ds2['test'][\"query-id\"], ds2['test'][\"corpus-id\"]):\n",
    "    query_to_corpus[query_id].append(corpus_id)\n",
    "    queries_list.append(query_id)\n",
    "    corpus_list.append(corpus_id)\n",
    "\n",
    "query_to_corpus = dict(query_to_corpus)\n",
    "query_to_corpus[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Poproś o ponowne wystawienie czeku właściwemu odbiorcy.']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dict = defaultdict(list)\n",
    "\n",
    "for item in corpus:\n",
    "  if int(item['_id']) in corpus_list:\n",
    "    corpus_dict[int(item['_id'])].append(item['text'])\n",
    "\n",
    "corpus_dict = dict(corpus_dict)\n",
    "corpus_dict[566392]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Po prostu poproś współpracownika o podpisanie odwrotu, a następnie zdeponowanie go. Nazywa się to czekiem strony trzeciej i jest całkowicie legalne. Nie zdziwiłbym się, gdyby czek był dłuższy i, jak zawsze, nie dostaniesz pieniędzy, jeśli czek nie zostanie zrealizowany. Teraz możesz mieć problemy, jeśli jest to duża kwota lub nie jesteś zbyt dobrze znany w banku. W takim przypadku możesz poprosić współpracownika o udanie się do banku i zatwierdzenie go przed kasjerem za pomocą dowodu tożsamości. Technicznie nawet nie musisz tam być. Każdy może wpłacić pieniądze na Twoje konto, jeśli ma numer konta. Mógł też po prostu wpłacić go na swoje konto i wypisać czek na firmę.']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dict[65404]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jak zdeponować czek wystawiony na współpracownika w mojej firmie na moje konto firmowe?']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_dict = defaultdict(list)\n",
    "\n",
    "for item in queries:\n",
    "  if int(item['_id']) in queries_list:\n",
    "    queries_dict[int(item['_id'])].append(item['text'])\n",
    "\n",
    "queries_dict = dict(queries_dict)\n",
    "queries_dict[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) prepering the lemmatization indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_config_lamentizer = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"polish_with_synonyms_lam\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"synonym_filter\",\n",
    "            \"morfologik_stem\",\n",
    "            \"lowercase\"\n",
    "          ]\n",
    "        },\n",
    "        \"polish_without_synonyms_lam\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"morfologik_stem\",\n",
    "            \"lowercase\"\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"synonym_filter\": {\n",
    "          \"type\": \"synonym\",\n",
    "          \"synonyms\": [\"kwiecień, kwi, IV\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"answer_syn\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_with_synonyms_lam\"\n",
    "      },\n",
    "      \"answer_without_sym\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_without_synonyms_lam\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name_lam = index_name + '_lam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docs_lam(dict):\n",
    "    for key, value in dict.items():\n",
    "        yield {\n",
    "            \"_index\": index_name_lam,\n",
    "            \"_id\": key,\n",
    "            \"_source\": {\n",
    "                \"answer_syn\": value,\n",
    "                \"answer_without_sym\": value,\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 1706 documents\n"
     ]
    }
   ],
   "source": [
    "es.options(ignore_status=[400, 404]).indices.delete(index=index_name_lam)\n",
    "es.indices.create(index=index_name_lam, body=index_config_lamentizer)\n",
    "\n",
    "try:\n",
    "    success, errors = helpers.bulk(es, generate_docs_lam(corpus_dict))\n",
    "    \n",
    "    print(f\"Successfully indexed {success} documents\")\n",
    "    if errors:\n",
    "        print(f\"Errors during indexing: {errors}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during bulk indexing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_syn = defaultdict(list)\n",
    "responses = defaultdict(list)\n",
    "for key, value  in queries_dict.items():\n",
    "    query_with_synonyms = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_syn\": value[0]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "\n",
    "    query_without_synonyms = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_without_sym\": value[0]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "\n",
    "    response = es.search(index=index_name_lam, body=query_with_synonyms)\n",
    "    responses_syn[key] = response['hits']['hits']\n",
    "    response = es.search(index=index_name_lam, body=query_without_synonyms)\n",
    "    responses[key] = response['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gdzie powinienem zaparkować mój fundusz na deszczowy dzień / awaryjny?']\n",
      "['Jak na razie świetne odpowiedzi, więc dodam jeszcze tylko jedną uwagę: płynność. Pieniądze zainwestowane w fundusz inwestycyjny (z wyłączeniem kont emerytalnych z karami za wcześniejszą wypłatę) mają stosunkowo wysoką płynność. Podczas gdy nadwyżka kapitału w twoim domu z powodu wcześniejszej spłaty ma bardzo niską płynność. Mówiąc prościej: jeśli znajdziesz się w rozpaczliwej sytuacji (długotrwałe bezrobocie), lepiej jest spieniężyć fundusz powierniczy, niż próbować szybko sprzedać swój dom i zamieszkać z matką. Płynność staje się mniejszym problemem, jeśli uda Ci się również sfinansować przyzwoity fundusz na deszczowe dni (6-9 miesięcy wydatków na życie).']\n"
     ]
    }
   ],
   "source": [
    "responses_syn = dict(responses_syn)\n",
    "responses = dict(responses)\n",
    "print(queries_dict[4641])\n",
    "print(responses[4641][0]['_source']['answer_syn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_ndcg(outputs):\n",
    "  ndcg5_list = []\n",
    "\n",
    "  for key, value in queries_dict.items():\n",
    "\n",
    "    corpuse_ids = [int(value_syn['_id']) for value_syn  in outputs[key]]\n",
    "    scores = np.asarray([value_syn['_score'] for value_syn  in outputs[key]])\n",
    "\n",
    "    true_relevance = np.asarray([1 if item in query_to_corpus[key] else 0 for item in corpuse_ids])\n",
    "    true_relevance = true_relevance.reshape(1, -1)\n",
    "    scores = scores.reshape(1, -1)\n",
    "\n",
    "    ndcg = ndcg_score(true_relevance, scores, k=5)\n",
    "\n",
    "    ndcg5_list.append(ndcg)\n",
    "\n",
    "  return np.mean(ndcg5_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 for synonymous 0.516269528610264\n",
      "NDCG@5 without synonymous 0.516269528610264\n"
     ]
    }
   ],
   "source": [
    "ndcg_lam_synonym = calculate_ndcg(responses_syn)\n",
    "ndcg_lam_no_synonym = calculate_ndcg(responses)\n",
    "print(f\"NDCG@5 for synonymous {ndcg_lam_synonym}\")\n",
    "print(f\"NDCG@5 without synonymous {ndcg_lam_no_synonym}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Prepering no lamantaizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_config_no_lamentizer = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"polish_with_synonyms\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"synonym_filter\",\n",
    "            \"morfologik_stem\",\n",
    "          ]\n",
    "        },\n",
    "        \"polish_without_synonyms\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"morfologik_stem\"\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"synonym_filter\": {\n",
    "          \"type\": \"synonym\",\n",
    "          \"synonyms\": [\"kwiecień, kwi, IV\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"answer_syn\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_with_synonyms\"\n",
    "      },\n",
    "      \"answer_without_sym\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_without_synonyms\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biore_sie_do_roboty_no_lam_2'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name_no_lam = index_name + '_no_lam_2'\n",
    "index_name_no_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docs_lam(dict):\n",
    "    for key, value in dict.items():\n",
    "        yield {\n",
    "            \"_index\": index_name_no_lam,\n",
    "            \"_id\": key,\n",
    "            \"_source\": {\n",
    "                \"answer_syn\": value,\n",
    "                \"answer_without_sym\": value,\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.options(ignore_status=[400, 404]).indices.delete(index=index_name_lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 1706 documents\n"
     ]
    }
   ],
   "source": [
    "es.options(ignore_status=[400, 404]).indices.delete(index=index_name_no_lam)\n",
    "es.indices.create(index=index_name_no_lam, body=index_config_no_lamentizer)\n",
    "\n",
    "try:\n",
    "    success, errors = helpers.bulk(es, generate_docs_lam(corpus_dict))\n",
    "    \n",
    "    print(f\"Successfully indexed {success} documents\")\n",
    "    if errors:\n",
    "        print(f\"Errors during indexing: {errors}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during bulk indexing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_syn = defaultdict(list)\n",
    "responses = defaultdict(list)\n",
    "for key, value  in queries_dict.items():\n",
    "    query_with_synonyms = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_syn\": value[0]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "\n",
    "    query_without_synonyms = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_without_sym\": value[0]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "\n",
    "    response = es.search(index=index_name_no_lam, body=query_with_synonyms)\n",
    "    responses_syn[key] = response['hits']['hits']\n",
    "    response = es.search(index=index_name_no_lam, body=query_without_synonyms)\n",
    "    responses[key] = response['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 for synonymous no lamentaizer 0.516269528610264\n",
      "NDCG@5 without synonymous no lamentaizer 0.516269528610264\n",
      "NDCG@5 for synonymous with lamentaizer 0.516269528610264\n",
      "NDCG@5 without synonymous with  lamentaizer 0.516269528610264\n"
     ]
    }
   ],
   "source": [
    "ndcg_no_lam_synonym = calculate_ndcg(responses_syn)\n",
    "ndcg_no_lam_no_synonym = calculate_ndcg(responses)\n",
    "print(f\"NDCG@5 for synonymous no lamentaizer {ndcg_no_lam_synonym}\")\n",
    "print(f\"NDCG@5 without synonymous no lamentaizer {ndcg_no_lam_no_synonym}\")\n",
    "print(f\"NDCG@5 for synonymous with lamentaizer {ndcg_lam_synonym}\")\n",
    "print(f\"NDCG@5 without synonymous with  lamentaizer {ndcg_lam_no_synonym}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the strengths and weaknesses of regular expressions versus full text search regarding processing of text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of full-text search with Elasticsearch lies in its indexing capability. Words are indexed just once, allowing us to efficiently run various queries with different terms. However, this strength can also be its greatest limitation—any new text added to documents requires reindexing, and deletions necessitate updates to the index.\n",
    "\n",
    "On the other hand, regex shines in its simplicity. It is straightforward to write and easy to use, making it a better choice when dealing with smaller amounts of text where indexing overhead is unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can an LLM be applied in the context of searching for documents? Justify your answer, excluding the obvious observation that an LLM can be used to formulate the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes LLM can be used for searching for text in documents. \n",
    "- LLM can help deveolping regex queries or even optimizing\n",
    "- Can extract key entities from text like NER\n",
    "- Searching for keywords like regex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
