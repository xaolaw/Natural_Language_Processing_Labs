{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Loading fiqa-pl corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    corpus: Dataset({\n",
       "        features: ['_id', 'title', 'text'],\n",
       "        num_rows: 57638\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'title', 'text'],\n",
       "    num_rows: 57638\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ds['corpus']\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Preparing elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a link for a local elasticsearch, beacuase i disabled ssl in elastic config we do not need to link to certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'http://localhost:9200/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch(['http://localhost:9200'])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(link)\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the index \n",
    "- First we need analyzer one with synonym filter and one without. Both have to have lowercase and morfologik_stem filter\n",
    "- Secondly filter that defines the synonyms for Polish month `kwiecień`\n",
    "- Thirdly mappings one for each anylzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_config = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"polish_with_synonyms\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"synonym_filter\",\n",
    "                        \"morfologik_stem\"\n",
    "                    ]\n",
    "                },\n",
    "                \"polish_without_synonyms\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"morfologik_stem\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"synonym_filter\": {\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": [\"kwiecień, kwi, IV\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content_with_synonyms\": {\"type\": \"text\", \"analyzer\": \"polish_with_synonyms\"},\n",
    "            \"content_without_synonyms\": {\"type\": \"text\", \"analyzer\": \"polish_without_synonyms\"}\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare texts/documents for elastic indexing. We loop through corpus and index every text. Thanks to `helpers.bulk()` function we can efficiently index multiple documents in a single API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import helpers\n",
    "\n",
    "index_name =  \"biore_sie_do_roboty\"\n",
    "\n",
    "def generate_docs(ds):\n",
    "    for doc in ds:\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": doc[\"_id\"],\n",
    "            \"_source\": {\n",
    "                \"content_with_synonyms\": doc[\"text\"], \n",
    "                \"content_without_synonyms\": doc[\"text\"]\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also delete and add the index so that we always create a new on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 57638 documents\n"
     ]
    }
   ],
   "source": [
    "es.options(ignore_status=[400, 404]).indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, body=index_config)\n",
    "\n",
    "try:\n",
    "    success, errors = helpers.bulk(es, generate_docs(corpus))\n",
    "    \n",
    "    print(f\"Successfully indexed {success} documents\")\n",
    "    if errors:\n",
    "        print(f\"Errors during indexing: {errors}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during bulk indexing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the queries for `kwiecień` and see how many times it occures in corpus with and without synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of occurrences of the word 'kwiecień' including its synonyms: 306\n",
      "The number of occurrences of the word 'kwiecień' without synonyms: 257\n"
     ]
    }
   ],
   "source": [
    "query_with_synonyms = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content_with_synonyms\": \"kwiecień\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "query_without_synonyms = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content_without_synonyms\": \"kwiecień\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=index_name, body=query_with_synonyms)\n",
    "print(\"The number of occurrences of the word 'kwiecień' including its synonyms:\",response['hits']['total']['value'])\n",
    "response = es.search(index=index_name, body=query_without_synonyms)\n",
    "print(\"The number of occurrences of the word 'kwiecień' without synonyms:\",response['hits']['total']['value'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Using fiqa-pl-qrels dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Preparing the qa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 14166\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 1238\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 1706\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2 = load_dataset(\"clarin-knext/fiqa-pl-qrels\")\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'title', 'text'],\n",
       "    num_rows: 6648\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds3 = load_dataset(\"clarin-knext/fiqa-pl\", \"queries\")\n",
    "queries = ds3['queries']\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '3',\n",
       " 'title': '',\n",
       " 'text': 'Nie mówię, że nie podoba mi się też pomysł szkolenia w miejscu pracy, ale nie możesz oczekiwać, że firma to zrobi. Szkolenie pracowników to nie ich praca – oni tworzą oprogramowanie. Być może systemy edukacyjne w Stanach Zjednoczonych (lub ich studenci) powinny trochę martwić się o zdobycie umiejętności rynkowych w zamian za ich ogromne inwestycje w edukację, zamiast wychodzić z tysiącami zadłużonych studentów i narzekać, że nie są do niczego wykwalifikowani.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[566392, 65404]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "corpus_list = []\n",
    "queries_list = []\n",
    "query_to_corpus = defaultdict(list)\n",
    "\n",
    "for query_id, corpus_id in zip(ds2['test'][\"query-id\"], ds2['test'][\"corpus-id\"]):\n",
    "    query_to_corpus[query_id].append(corpus_id)\n",
    "    queries_list.append(query_id)\n",
    "    corpus_list.append(corpus_id)\n",
    "\n",
    "query_to_corpus = dict(query_to_corpus)\n",
    "#Answer to query 8\n",
    "query_to_corpus[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Poproś o ponowne wystawienie czeku właściwemu odbiorcy.']\n",
      "['Po prostu poproś współpracownika o podpisanie odwrotu, a następnie zdeponowanie go. Nazywa się to czekiem strony trzeciej i jest całkowicie legalne. Nie zdziwiłbym się, gdyby czek był dłuższy i, jak zawsze, nie dostaniesz pieniędzy, jeśli czek nie zostanie zrealizowany. Teraz możesz mieć problemy, jeśli jest to duża kwota lub nie jesteś zbyt dobrze znany w banku. W takim przypadku możesz poprosić współpracownika o udanie się do banku i zatwierdzenie go przed kasjerem za pomocą dowodu tożsamości. Technicznie nawet nie musisz tam być. Każdy może wpłacić pieniądze na Twoje konto, jeśli ma numer konta. Mógł też po prostu wpłacić go na swoje konto i wypisać czek na firmę.']\n"
     ]
    }
   ],
   "source": [
    "corpus_dict = defaultdict(list)\n",
    "\n",
    "for item in corpus:\n",
    "  if int(item['_id']) in corpus_list:\n",
    "    corpus_dict[int(item['_id'])].append(item['text'])\n",
    "\n",
    "corpus_dict = dict(corpus_dict)\n",
    "print(corpus_dict[566392])\n",
    "print(corpus_dict[65404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jak zdeponować czek wystawiony na współpracownika w mojej firmie na moje konto firmowe?']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_dict = defaultdict(list)\n",
    "\n",
    "for item in queries:\n",
    "  if int(item['_id']) in queries_list:\n",
    "    queries_dict[int(item['_id'])].append(item['text'])\n",
    "\n",
    "queries_dict = dict(queries_dict)\n",
    "queries_dict[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) prepering the lemmatization and synonymous indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_config_lamentizer = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"polish_with_synonyms_with_lam\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"morfologik_stem\",\n",
    "            \"lowercase\"\n",
    "          ]\n",
    "        },\n",
    "        \"polish_with_synonyms_without_lam\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"morfologik_stem\",\n",
    "          ]\n",
    "        },\n",
    "        \"polish_without_synonyms_with_lam\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"lowercase\",\n",
    "          ]\n",
    "        },\n",
    "        \"polish_without_synonyms_without_lam\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"answer_with_synonyms_with_lam\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_with_synonyms_with_lam\"\n",
    "      },\n",
    "      \"answer_with_synonyms_without_lam\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_with_synonyms_without_lam\"\n",
    "      },\n",
    "      \"answer_without_synonyms_with_lam\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_without_synonyms_with_lam\"\n",
    "      },\n",
    "      \"answer_without_synonyms_without_lam\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_without_synonyms_without_lam\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name_lam = index_name + '_lam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docs_lam(ds):\n",
    "    for doc in ds:\n",
    "        yield {\n",
    "            \"_index\": index_name_lam,\n",
    "            \"_id\": doc[\"_id\"],\n",
    "            \"_source\": {\n",
    "                \"answer_with_synonyms_with_lam\": doc[\"text\"],\n",
    "                \"answer_with_synonyms_without_lam\": doc[\"text\"],\n",
    "                \"answer_without_synonyms_with_lam\": doc['text'],\n",
    "                \"answer_without_synonyms_without_lam\": doc['text']\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 57638 documents\n"
     ]
    }
   ],
   "source": [
    "es.options(ignore_status=[400, 404]).indices.delete(index=index_name_lam)\n",
    "es.indices.create(index=index_name_lam, body=index_config_lamentizer)\n",
    "\n",
    "try:\n",
    "    success, errors = helpers.bulk(es, generate_docs_lam(corpus))\n",
    "    \n",
    "    print(f\"Successfully indexed {success} documents\")\n",
    "    if errors:\n",
    "        print(f\"Errors during indexing: {errors}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during bulk indexing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_with_synonyms_with_lam': {},\n",
       " 'answer_with_synonyms_without_lam': {},\n",
       " 'answer_without_synonyms_with_lam': {},\n",
       " 'answer_without_synonyms_without_lam': {}}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lam_syn = {}\n",
    "\n",
    "indexes_types = [\n",
    "  \"answer_with_synonyms_with_lam\",\n",
    "  \"answer_with_synonyms_without_lam\",\n",
    "  \"answer_without_synonyms_with_lam\",\n",
    "  \"answer_without_synonyms_without_lam\"\n",
    "]\n",
    "for i in indexes_types:\n",
    "  results_lam_syn[i] = {}\n",
    "\n",
    "results_lam_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value  in queries_dict.items():\n",
    "    query_with_synonyms_with_lam = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_with_synonyms_with_lam\": value[0]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "    query_with_synonyms_without_lam = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_with_synonyms_without_lam\": value[0]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "    query_without_synonyms_with_lam = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_without_synonyms_with_lam\": value[0]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "    query_without_synonyms_without_lam = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_without_synonyms_without_lam\": value[0]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "\n",
    "    queries_temp_list = [query_with_synonyms_with_lam,\n",
    "                         query_with_synonyms_without_lam,\n",
    "                         query_without_synonyms_with_lam,\n",
    "                         query_without_synonyms_without_lam]\n",
    "\n",
    "    for j in range(len(queries_temp_list)):\n",
    "        response = es.search(index=index_name_lam, body=queries_temp_list[j])\n",
    "        response = response['hits']['hits']\n",
    "        temp_list = []\n",
    "        for i in response:\n",
    "            temp_list.append(int(i['_id']))\n",
    "        results_lam_syn[indexes_types[j]][key] = temp_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[376148, 253614, 580025, 497993, 32833]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lam_syn[indexes_types[0]][4641]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gdzie powinienem zaparkować mój fundusz na deszczowy dzień / awaryjny?']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_dict[4641]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDCG implemented from https://en.wikipedia.org/wiki/Discounted_cumulative_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_dcg(documents_relevance, k):\n",
    "  sum = 0\n",
    "  for index in range(k):\n",
    "    #need to add another + 1 because python lists starts from 0\n",
    "    sum += documents_relevance[index] / np.log2(index + 1 + 1)\n",
    "\n",
    "  return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndcg(results, query_to_corpus, k):\n",
    "  ndcg_list = []\n",
    "  for key, items in results.items():\n",
    "    true_relevance = [1 if i in query_to_corpus[key] else 0 for i in items]\n",
    "    \n",
    "    dcg = calculate_dcg(true_relevance, k)\n",
    "    idcg = calculate_dcg(sorted(true_relevance, reverse=True), k)\n",
    "\n",
    "    ndcg_list.append(0 if dcg == 0 else dcg / idcg)\n",
    "  return np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785002371969948\n"
     ]
    }
   ],
   "source": [
    "dcg = calculate_dcg([3,2,3,0,1,2], 6)\n",
    "idcg = calculate_dcg(sorted([3,2,3,0,1,2,3,2], reverse=True), 6)\n",
    "print(dcg/idcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 for answer_with_synonyms_with_lam: 0.2657322972429154\n",
      "NDCG@5 for answer_with_synonyms_without_lam: 0.2657322972429154\n",
      "NDCG@5 for answer_without_synonyms_with_lam: 0.20782902393038719\n",
      "NDCG@5 for answer_without_synonyms_without_lam: 0.20782902393038719\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(results_lam_syn)):\n",
    "  result = calculate_ndcg(results_lam_syn[indexes_types[j]],query_to_corpus, 5)\n",
    "  print(f\"NDCG@5 for {indexes_types[j]}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the analizers without synonyms suffer great disadvantage against those with them, also lamantazizer seems to not influence the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the strengths and weaknesses of regular expressions versus full text search regarding processing of text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of full-text search with Elasticsearch lies in its indexing capability. Words are indexed just once, allowing us to efficiently run various queries with different terms. However, this strength can also be its greatest limitation—any new text added to documents requires reindexing, and deletions necessitate updates to the index.\n",
    "\n",
    "On the other hand, regex shines in its simplicity. It is straight forward to write and easy to use, making it a better choice when dealing with smaller amounts of text where indexing overhead is unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can an LLM be applied in the context of searching for documents? Justify your answer, excluding the obvious observation that an LLM can be used to formulate the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes LLM can be used for searching for text in documents. \n",
    "- LLM can help deveolping regex queries or even optimizing\n",
    "- Can extract key entities from text like NER\n",
    "- Searching for keywords like regex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
