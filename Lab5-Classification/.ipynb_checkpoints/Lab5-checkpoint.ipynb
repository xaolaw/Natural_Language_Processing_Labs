{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Loading corpus and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")\n",
    "ds2 = load_dataset(\"clarin-knext/fiqa-pl\", \"queries\")\n",
    "corpus = ds['corpus'].to_pandas()\n",
    "queries = ds2['queries'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Co jest uwaÅ¼ane za wydatek sÅ‚uÅ¼bowy w podrÃ³Å¼y ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>Wydatki sÅ‚uÅ¼bowe - ubezpieczenie samochodu pod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>RozpoczÄ™cie nowego biznesu online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>â€DzieÅ„ roboczyâ€ i â€termin pÅ‚atnoÅ›ciâ€ rachunkÃ³w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>Nowy wÅ‚aÅ›ciciel firmy â€“ Jak dziaÅ‚ajÄ… podatki d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  _id title                                               text\n",
       "0   0        Co jest uwaÅ¼ane za wydatek sÅ‚uÅ¼bowy w podrÃ³Å¼y ...\n",
       "1   4        Wydatki sÅ‚uÅ¼bowe - ubezpieczenie samochodu pod...\n",
       "2   5                        RozpoczÄ™cie nowego biznesu online\n",
       "3   6           â€DzieÅ„ roboczyâ€ i â€termin pÅ‚atnoÅ›ciâ€ rachunkÃ³w\n",
       "4   7        Nowy wÅ‚aÅ›ciciel firmy â€“ Jak dziaÅ‚ajÄ… podatki d..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Creating a dataset of positive and negative sentence pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the dataset that conects queries with corpus by id, question and answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 14166\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 1238\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 1706\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds3 = load_dataset(\"clarin-knext/fiqa-pl-qrels\")\n",
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = ds3['validation'].to_pandas()\n",
    "train = ds3['train'].to_pandas()\n",
    "test = ds3['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>308938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>296717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>314352</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query-id  corpus-id  score\n",
       "0         1      14255      1\n",
       "1         2     308938      1\n",
       "2         3     296717      1\n",
       "3         3     100764      1\n",
       "4         3     314352      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see the question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  ZgÅ‚aszanie wydatkÃ³w biznesowych dla firmy bez dochodu\n",
      "Answer:  Tak, moÅ¼esz ubiegaÄ‡ siÄ™ o potrÄ…cenia biznesowe, jeÅ›li nie uzyskujesz jeszcze Å¼adnych dochodÃ³w. Ale najpierw powinieneÅ› zdecydowaÄ‡, jakÄ… strukturÄ™ chcesz mieÄ‡ dla swojej firmy. Albo struktura firmy, albo jednoosobowy przedsiÄ™biorca lub spÃ³Å‚ka osobowa. Struktura firmy JeÅ›li wybierzesz strukturÄ™ firmy (ktÃ³ra jest droÅ¼sza w przygotowaniu), bÄ™dziesz ubiegaÄ‡ siÄ™ o potrÄ…cenia, ale bez dochodu. WiÄ™c ponosiÅ‚byÅ› stratÄ™ i kontynuowaÅ‚byÅ› straty, dopÃ³ki dochÃ³d z firmy nie przekroczy twoich wydatkÃ³w. Straty te pozostanÄ… wiÄ™c w firmie i bÄ™dÄ… mogÅ‚y zostaÄ‡ przeniesione na przyszÅ‚e lata dochodowe, kiedy bÄ™dziesz osiÄ…gaÄ‡ zyski, aby zrekompensowaÄ‡ te zyski. WiÄ™cej informacji moÅ¼na znaleÅºÄ‡ w ATO â€“ Straty podatkowe firmy. PrzedsiÄ™biorca jednoosobowy o strukturze partnerskiej JeÅ›li zdecydujesz siÄ™ byÄ‡ jednoosobowym przedsiÄ™biorcÄ… lub spÃ³Å‚kÄ… osobowÄ…, a Twoja firma ponosi straty, musisz sprawdziÄ‡ zasady dotyczÄ…ce strat niekomercyjnych, aby sprawdziÄ‡, czy moÅ¼esz zrÃ³wnowaÅ¼yÄ‡ stratÄ™ z dochodami z innych ÅºrÃ³deÅ‚, takich jak zarobki. Aby zrekompensowaÄ‡ straty firmy z innymi dochodami, firma musi zdaÄ‡ jeden z tych testÃ³w: JeÅ›li nie zdasz Å¼adnego z tych testÃ³w, co najprawdopodobniej nie zostanie wykonane jako start-up, musisz kontynuowaÄ‡ dziaÅ‚alnoÅ›Ä‡ straty aÅ¼ do roku dochodowego, w ktÃ³rym zdasz jeden z testÃ³w, wtedy moÅ¼esz odliczyÄ‡ je od innych dochodÃ³w. To wÅ‚aÅ›nie odrÃ³Å¼nia legalnÄ… firmÄ™ od kogoÅ›, kto ma hobby, poniewaÅ¼ jeÅ›li nie zaczniesz zarabiaÄ‡ co najmniej 20 000 USD ze sprzedaÅ¼y (najÅ‚atwiejszy test do zdawania), nie moÅ¼esz wykorzystaÄ‡ strat biznesowych w stosunku do innych dochodÃ³w. WiÄ™cej informacji moÅ¼na znaleÅºÄ‡ w ATO â€“ Straty niekomercyjne.\n"
     ]
    }
   ],
   "source": [
    "print(\"Question: \", queries[queries['_id'] == '1']['text'].iloc[0])\n",
    "print(\"Answer: \", corpus[corpus['_id'] == '14255']['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7578, -0.6469],\n",
      "        [-0.7338,  1.0468]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "trained_model = True\n",
    "model_path = \"\"\n",
    "model = None\n",
    "\n",
    "if trained_model:\n",
    "    model_path = \"../trained_model\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path\n",
    "        ).to(\"cuda\")\n",
    "else:\n",
    "    model_path = \"clarin-knext/herbert-base-reranker-msmarco\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path\n",
    "        ).to(\"cuda\")\n",
    "    model.classifier = nn.Linear(768, 2, device=model.device)\n",
    "    model.num_labels = 2\n",
    "    model.config.num_labels = 2\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "features = tokenizer(['Jakie miasto jest stolica Polski?', 'StolicÄ… Polski jest Warszawa.'],  padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separator = tokenizer.sep_token\n",
    "separator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "big_df = pd.concat([validation,train,test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'ZgÅ‚aszanie wydatkÃ³w biznesowych dla firmy bez dochodu</s>Tak, moÅ¼esz ubiegaÄ‡ siÄ™ o potrÄ…cenia biznesowe, jeÅ›li nie uzyskujesz jeszcze Å¼adnych dochodÃ³w. Ale najpierw powinieneÅ› zdecydowaÄ‡, jakÄ… strukturÄ™ chcesz mieÄ‡ dla swojej firmy. Albo struktura firmy, albo jednoosobowy przedsiÄ™biorca lub spÃ³Å‚ka osobowa. Struktura firmy JeÅ›li wybierzesz strukturÄ™ firmy (ktÃ³ra jest droÅ¼sza w przygotowaniu), bÄ™dziesz ubiegaÄ‡ siÄ™ o potrÄ…cenia, ale bez dochodu. WiÄ™c ponosiÅ‚byÅ› stratÄ™ i kontynuowaÅ‚byÅ› straty, dopÃ³ki dochÃ³d z firmy nie przekroczy twoich wydatkÃ³w. Straty te pozostanÄ… wiÄ™c w firmie i bÄ™dÄ… mogÅ‚y zostaÄ‡ przeniesione na przyszÅ‚e lata dochodowe, kiedy bÄ™dziesz osiÄ…gaÄ‡ zyski, aby zrekompensowaÄ‡ te zyski. WiÄ™cej informacji moÅ¼na znaleÅºÄ‡ w ATO â€“ Straty podatkowe firmy. PrzedsiÄ™biorca jednoosobowy o strukturze partnerskiej JeÅ›li zdecydujesz siÄ™ byÄ‡ jednoosobowym przedsiÄ™biorcÄ… lub spÃ³Å‚kÄ… osobowÄ…, a Twoja firma ponosi straty, musisz sprawdziÄ‡ zasady dotyczÄ…ce strat niekomercyjnych, aby sprawdziÄ‡, czy moÅ¼esz zrÃ³wnowaÅ¼yÄ‡ stratÄ™ z dochodami z innych ÅºrÃ³deÅ‚, takich jak zarobki. Aby zrekompensowaÄ‡ straty firmy z innymi dochodami, firma musi zdaÄ‡ jeden z tych testÃ³w: JeÅ›li nie zdasz Å¼adnego z tych testÃ³w, co najprawdopodobniej nie zostanie wykonane jako start-up, musisz kontynuowaÄ‡ dziaÅ‚alnoÅ›Ä‡ straty aÅ¼ do roku dochodowego, w ktÃ³rym zdasz jeden z testÃ³w, wtedy moÅ¼esz odliczyÄ‡ je od innych dochodÃ³w. To wÅ‚aÅ›nie odrÃ³Å¼nia legalnÄ… firmÄ™ od kogoÅ›, kto ma hobby, poniewaÅ¼ jeÅ›li nie zaczniesz zarabiaÄ‡ co najmniej 20 000 USD ze sprzedaÅ¼y (najÅ‚atwiejszy test do zdawania), nie moÅ¼esz wykorzystaÄ‡ strat biznesowych w stosunku do innych dochodÃ³w. WiÄ™cej informacji moÅ¼na znaleÅºÄ‡ w ATO â€“ Straty niekomercyjne.',\n",
       "  'label': 1},\n",
       " {'sentence': 'Przekazywanie pieniÄ™dzy z jednej kontroli biznesowej do innej kontroli biznesowej</s>â€PowinieneÅ› mieÄ‡ oddzielne pliki dla kaÅ¼dej z dwÃ³ch firm. Firma, ktÃ³ra przekazuje pieniÄ…dze, powinna â€â€wypisaÄ‡ czekâ€â€ w swoim pliku QB. Firma, ktÃ³ra otrzymuje pieniÄ…dze, powinna â€â€zrobiÄ‡ depozytâ€â€ w swoim pliku QB. QB â€wypisz czekâ€ nawet wtedy, gdy dokonujesz pÅ‚atnoÅ›ci w inny sposÃ³b, np. ACH). Å»adna firma nie powinna mieÄ‡ wyraÅºnie reprezentowanych kont bankowych drugiej strony. Z kaÅ¼dej strony musisz rÃ³wnieÅ¼ zaklasyfikowaÄ‡ pÅ‚atnoÅ›Ä‡ jako pochodzÄ…cÄ… z/przeszedÅ‚ na inne konto â€“ aby wiedzieÄ‡, co jest tam poprawne, musimy najpierw wiedzieÄ‡, dlaczego przelewasz pieniÄ…dze i w jaki sposÃ³b masz ustalone ksiÄ…Å¼ki. MyÅ›lÄ™, Å¼e to prawdopodobnie wykracza poza zakres tego, co jest na temat / wykonalne tutaj. PieniÄ…dze z twojego konta osobistego sÄ… prawdopodobnie kapitaÅ‚em wÅ‚asnym wÅ‚aÅ›ciciela, chyba Å¼e dzieje siÄ™ coÅ› innego. Na przykÅ‚ad w S Corp powinieneÅ› pÅ‚aciÄ‡ sobie pensjÄ™. JeÅ›li przez przypadek przepÅ‚acisz, moÅ¼esz napisaÄ‡ czek z powrotem do firmy z konta osobistego, aby poprawiÄ‡ bÅ‚Ä…d. To nie jest kapitaÅ‚ wÅ‚asny â€” to prawdopodobnie â€wydatek ujemnyâ€ na innym koncie, ktÃ³re Å›ledzi wypÅ‚aty wynagrodzeniaâ€.',\n",
       "  'label': 1},\n",
       " {'sentence': 'Posiadanie oddzielnego konta bankowego do prowadzenia dziaÅ‚alnoÅ›ci/inwestowania, ale nie â€konta firmowegoâ€?</s>â€Posiadanie oddzielnego konta czekowego dla firmy ma sens. UÅ‚atwia to dokumentowanie dochodÃ³w/wydatkÃ³w. MoÅ¼esz â€wyjaÅ›niÄ‡â€ kaÅ¼dy dolar wchodzÄ…cy i wychodzÄ…cy z konta bez koniecznoÅ›ci pamiÄ™tania, Å¼e \\u200b\\u200bniektÃ³re z nich byÅ‚y przeznaczone na pozycje niebiznesowe. Unia kredytowa pozwoliÅ‚a mi mieÄ‡ drugie konto czekowe i pozwoliÅ‚a mi umieÅ›ciÄ‡ to, co chciaÅ‚em jako imiÄ™ na czeku.MyÅ›lÄ™, Å¼e wyglÄ…daÅ‚o to trochÄ™ lepiej niÅ¼ posiadanie mojego imienia i nazwiska na czeku.Nie widzÄ™ potrzeby oddzielnego rachunek bieÅ¼Ä…cy do inwestowania PieniÄ…dze mogÄ… byÄ‡ trzymane na oddzielnym koncie oszczÄ™dnoÅ›ciowym, ktÃ³re nie ma Å¼adnych opÅ‚at, a nawet moÅ¼e trochÄ™ zarobiÄ‡. Chyba Å¼e robisz duÅ¼o transakcji inwestycyjnych w miesiÄ…cu, to mi siÄ™ udaÅ‚o. FinansujÄ™ IRA i 529 planuje w ten sposÃ³b. Otrzymujemy czeki 4-5 razy w miesiÄ…cu, ale pieniÄ…dze wysyÅ‚amy do kaÅ¼dego z funduszy raz w miesiÄ…cu. BÄ™dziesz potrzebowaÄ‡ konta firmowego, jeÅ›li liczba transakcji stanie siÄ™ duÅ¼a. JeÅ›li za kaÅ¼dym razem wpÅ‚acasz dziesiÄ…tki czekÃ³w do banku, bank bÄ™dzie chciaÅ‚ siÄ™ przenieÅ›Ä‡ y na konto firmowe.\"',\n",
       "  'label': 1},\n",
       " {'sentence': 'Posiadanie oddzielnego konta bankowego do prowadzenia dziaÅ‚alnoÅ›ci/inwestowania, ale nie â€konta firmowegoâ€?</s>â€Nie okreÅ›lasz, w jakim kraju siÄ™ znajdujesz, wiÄ™c moje odpowiedzi sÄ… bardziej z punktu widzenia najlepszych praktyk niÅ¼ z punktu widzenia prawa. Nie zamierzam uÅ¼ywaÄ‡ go do uÅ¼ytku osobistego, ale mam na myÅ›li, Å¼e jest tak jak to moÅ¼liwe. niebezpieczna propozycja.. Nie naleÅ¼y mieszaÄ‡ wydatkÃ³w sÅ‚uÅ¼bowych z osobistymi. JeÅ›li jest szansa, Å¼e \\u200b\\u200btak siÄ™ stanie, przestaÅ„, zrÃ³b to tak, aby tak siÄ™ nie staÅ‚o. DuÅ¼e niebezpieczeÅ„stwo polega na moÅ¼liwoÅ›ci Å›ledzenia miÄ™dzy tym, co robisz dla firmy, a tym, co robisz dla siebie. JeÅ›li uÅ¼ywasz tego konta jako â€tymczasowegoâ€ konta do inwestycji itp., czy sÄ… to inwestycje dla siebie? Czy dla firmy? Czy jest to opodatkowanie na zyskach kapitaÅ‚owych i/lub dywidendach sÄ… takie same dla osÃ³b prywatnych i firm w Twojej jurysdykcji? JeÅ›li kupisz widget, czy widget jest wydatkiem na dochÃ³d firmy? A moÅ¼e jest to wydatek z wÅ‚asnej kieszeni na konsumpcjÄ™ osobistÄ…? Ten pierwszy zmniejsza dochÃ³d podlegajÄ…cy opodatkowaniu , to drugie nie. Nie widzÄ™ korzyÅ›ci z posiadania prawdziwego konta firmowego, poniewaÅ¼ uÅ¼ywaj tych, ktÃ³re majÄ… cechy charakterystyczne dla korporacji, LLC itp. - nic korzystnego dla jedynego wÅ‚aÅ›ciciela, ktÃ³ry nie ma raportÃ³w / pracownikÃ³w. PrawdziwÄ… korzyÅ›ciÄ… jest to, Å¼e istnieje wyraÅºne rozgraniczenie miÄ™dzy dochodami/wydatkami biznesowymi a dochodami/wydatkami osobistymi. To konto moÅ¼e rÃ³wnieÅ¼ przyjmowaÄ‡ pieniÄ…dze i przechowywaÄ‡ je z transakcji biznesowych/sprzedaÅ¼y, a takÅ¼e ewentualnie przenosiÄ‡ czÄ™Å›Ä‡ na konto osobiste, jeÅ›li nie ma potrzeby reinwestowania tej kwoty/procentu. To, czego szukasz, to potocznie zwany rachunek bieÅ¼Ä…cy, poniewaÅ¼ sÅ‚uÅ¼y do bieÅ¼Ä…cych wydatkÃ³w. JeÅ›li przenosisz pieniÄ…dze z konta na konto osobiste, oznacza to pÅ‚acenie sobie, co ma rÃ³wnieÅ¼ inne konsekwencje. Najbezpieczniejszym/najczystszym sposobem, aby to zrobiÄ‡, jest: ChoÄ‡ moÅ¼e to brzmieÄ‡ jak przesada, jest to jedyny sposÃ³b, aby zagwarantowaÄ‡, Å¼e dochody/wydatki zostanÄ… przydzielone wÅ‚aÅ›ciwej jednostce (tj. Tobie lub Twojej firmie). Z kanadyjskiego punktu widzenia:\"',\n",
       "  'label': 1},\n",
       " {'sentence': 'Posiadanie oddzielnego konta bankowego do prowadzenia dziaÅ‚alnoÅ›ci/inwestowania, ale nie â€konta firmowegoâ€?</s>JeÅ›li to uÅ‚atwia finanse, dlaczego nie? Moja Å¼ona i ja mieliÅ›my jego/jej/nasz jeszcze zanim siÄ™ pobraliÅ›my. Mam rÃ³wnieÅ¼ konto do obsÅ‚ugi transakcji dotyczÄ…cych mojej wynajmowanej nieruchomoÅ›ci i jedno dodatkowe do korzystania z PayPal. MiaÅ‚em paranojÄ™, podajÄ…c numer konta czekowego z upowaÅ¼nieniem dla osoby trzeciej do obciÄ…Å¼enia go, tak aby konto miaÅ‚o maksymalnie kilkaset dolarÃ³w. Wszystko to ma na celu wyjaÅ›nienie, Å¼e twoje finanse powinny byÄ‡ tak zorganizowane, aby uproÅ›ciÄ‡ twoje Å¼ycie i zapewniÄ‡ ci wygodÄ™.',\n",
       "  'label': 1}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ = []\n",
    "\n",
    "for index, row in big_df.iterrows():\n",
    "\n",
    "  corpus_id = row['corpus-id']\n",
    "  query_id = row['query-id']\n",
    "\n",
    "  list_.append({\n",
    "    'sentence': queries[queries['_id'] == str(query_id)]['text'].iloc[0] + separator + corpus[corpus['_id'] == str(corpus_id)]['text'].iloc[0],\n",
    "    'label': 1\n",
    "  })\n",
    "\n",
    "list_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17110"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in big_df.iterrows():\n",
    "\n",
    "  corpus_id = row['corpus-id']\n",
    "  query_id = row['query-id']\n",
    "\n",
    "  for i in range(3):\n",
    "    \n",
    "    index = (corpus_id + i)%len(corpus)\n",
    "    if corpus.iloc[index]['_id'] != corpus_id:\n",
    "      list_.append({\n",
    "        'sentence': queries[queries['_id'] == str(query_id)]['text'].iloc[0] + separator + corpus.iloc[index]['text'],\n",
    "        'label': 0\n",
    "      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68440"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Training a text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe4a92316f34b62a648916418d775c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/41064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643191560c714e958664840eea4b3c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/41064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8113da8bcf34952b3fbc6d5fa46e9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/13688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169417243297456493341a82232396c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/13688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9366cc7294344b2e87b97753ff935a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/13688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a693cdb0ea024e4da2a6ded292506971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/13688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.DataFrame(list_, columns=['sentence', 'label'])\n",
    "\n",
    "temp, test = train_test_split(df, test_size=0.2, random_state=2137)\n",
    "train, eval = train_test_split(temp, test_size=0.25, random_state=2137)\n",
    "\n",
    "train = Dataset.from_pandas(train)\n",
    "train = train.class_encode_column(\"label\")\n",
    "test = Dataset.from_pandas(test)\n",
    "test = test.class_encode_column(\"label\")\n",
    "eval = Dataset.from_pandas(eval)\n",
    "eval = eval.class_encode_column(\"label\")\n",
    "\n",
    "train= train.rename_column(\"__index_level_0__\", \"input_ids\")\n",
    "test = test.rename_column(\"__index_level_0__\", \"input_ids\")\n",
    "eval = eval.rename_column(\"__index_level_0__\", \"input_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length:  41064  Test length:  13688  Evaluation:  13688\n"
     ]
    }
   ],
   "source": [
    "print(\"Train length: \", len(train), \" Test length: \", len(test), \" Evaluation: \", len(eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Co to jest odwrÃ³cenie niedÅºwiedziego paska?</s>Depilacja laserowa nie jest trwaÅ‚a, trzeba wykonaÄ‡ kilka sesji, aby usunÄ…Ä‡ wszystkie wÅ‚osy, kaÅ¼da sesja kosztuje setki. A potem moÅ¼e to potrwaÄ‡ tylko kilka lat, zanim wÅ‚osy odrosnÄ…. Nie jest wart swojej ceny, chyba Å¼e masz tak duÅ¼Ä… kwotÄ™, Å¼e moÅ¼esz wyrzuciÄ‡ kilka tysiÄ™cy. Jak powiedzieli inni, albo kupuj hurtowo ostrza, albo kup brzytwÄ™. BÄ™dÄ… znacznie bardziej ekonomiczne zarÃ³wno w krÃ³tkim, jak i dÅ‚ugim okresie.',\n",
       " 'label': 0,\n",
       " 'input_ids': 51985}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Studia\\NLP\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training\",\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=24,\n",
    "    per_device_train_batch_size=24\n",
    ")\n",
    "\n",
    "def compute_metric(eval_prediction):\n",
    "    predictions, labels = eval_prediction\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a27cc071094bef9a76e453349cc0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41064 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94eac4f221dc4f3d96c595c884783f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67718fc83334f2b99bb7c65eed66929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenizer_fn(batch):\n",
    "    return tokenizer(batch['sentence'], truncation=True, padding=True)\n",
    "\n",
    "tokenized_train = train.map(tokenizer_fn, batched=True)\n",
    "tokenized_eval = eval.map(tokenizer_fn, batched=True)\n",
    "tokenized_test = test.map(tokenizer_fn, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom trainer with weighted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch = None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 2 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 5.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adam6\\AppData\\Local\\Temp\\ipykernel_27776\\3114532008.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 13688\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Reporting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='571' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 95/571 00:44 < 03:45, 2.11 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Studia\\NLP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4076\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4073\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4075\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4076\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4077\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4079\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4080\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4086\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32md:\\Studia\\NLP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4270\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4267\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4270\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4271\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4272\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4274\u001b[0m )\n",
      "File \u001b[1;32md:\\Studia\\NLP\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4486\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   4484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[0;32m   4485\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4486\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   4487\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m   4489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m      6\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# compute custom loss (suppose one has 2 labels with different weights)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fct(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_labels), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find passage candidates using FTS, where the query is the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch(['http://localhost:9200'])>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "link = 'http://localhost:9200/'\n",
    "\n",
    "es = Elasticsearch(link)\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_config_lamentizer = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"polish_with_synonyms_with_lam\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"morfologik_stem\",\n",
    "            \"lowercase\"\n",
    "          ]\n",
    "        },\n",
    "        \"polish_with_synonyms_without_lam\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"morfologik_stem\",\n",
    "          ]\n",
    "        },\n",
    "        \"polish_without_synonyms_with_lam\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"lowercase\",\n",
    "          ]\n",
    "        },\n",
    "        \"polish_without_synonyms_without_lam\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"answer_with_synonyms_with_lam\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_with_synonyms_with_lam\"\n",
    "      },\n",
    "      \"answer_with_synonyms_without_lam\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_with_synonyms_without_lam\"\n",
    "      },\n",
    "      \"answer_without_synonyms_with_lam\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_without_synonyms_with_lam\"\n",
    "      },\n",
    "      \"answer_without_synonyms_without_lam\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"polish_without_synonyms_without_lam\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co jest uwaÅ¼ane za wydatek sÅ‚uÅ¼bowy w podrÃ³Å¼y sÅ‚uÅ¼bowej?\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for idx, value  in queries.iterrows():\n",
    "    print(value['text'])\n",
    "    print(value['_id'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'biore_sie_do_roboty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docs(ds):\n",
    "    for doc in ds:\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": doc[\"_id\"],\n",
    "            \"_source\": {\n",
    "                \"answer_with_synonyms_with_lam\": doc[\"text\"],\n",
    "                \"answer_with_synonyms_without_lam\": doc[\"text\"],\n",
    "                \"answer_without_synonyms_with_lam\": doc['text'],\n",
    "                \"answer_without_synonyms_without_lam\": doc['text']\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed 57638 documents\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import helpers\n",
    "\n",
    "es.options(ignore_status=[400, 404]).indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, body=index_config_lamentizer)\n",
    "\n",
    "try:\n",
    "    success, errors = helpers.bulk(es, generate_docs(ds['corpus']))\n",
    "    \n",
    "    print(f\"Successfully indexed {success} documents\")\n",
    "    if errors:\n",
    "        print(f\"Errors during indexing: {errors}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during bulk indexing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_with_synonyms_with_lam': {},\n",
       " 'answer_with_synonyms_without_lam': {},\n",
       " 'answer_without_synonyms_with_lam': {},\n",
       " 'answer_without_synonyms_without_lam': {}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lam_syn = {}\n",
    "\n",
    "indexes_types = [\n",
    "  \"answer_with_synonyms_with_lam\",\n",
    "  \"answer_with_synonyms_without_lam\",\n",
    "  \"answer_without_synonyms_with_lam\",\n",
    "  \"answer_without_synonyms_without_lam\"\n",
    "]\n",
    "for i in indexes_types:\n",
    "  results_lam_syn[i] = {}\n",
    "\n",
    "results_lam_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, value  in queries.iterrows():\n",
    "    query_with_synonyms_with_lam = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_with_synonyms_with_lam\": value['text']\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "    query_with_synonyms_without_lam = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_with_synonyms_without_lam\": value['text']\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "    query_without_synonyms_with_lam = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_without_synonyms_with_lam\": value['text']\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "    query_without_synonyms_without_lam = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"answer_without_synonyms_without_lam\": value['text']\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "\n",
    "    queries_temp_list = [query_with_synonyms_with_lam,\n",
    "                         query_with_synonyms_without_lam,\n",
    "                         query_without_synonyms_with_lam,\n",
    "                         query_without_synonyms_without_lam]\n",
    "\n",
    "    for j in range(len(queries_temp_list)):\n",
    "        response = es.search(index=index_name, body=queries_temp_list[j])\n",
    "        response = response['hits']['hits']\n",
    "        temp_list = []\n",
    "        for i in response:\n",
    "            temp_list.append(int(i['_id']))\n",
    "\n",
    "        results_lam_syn[indexes_types[j]][value['_id']] = temp_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_dcg(documents_relevance, k):\n",
    "  sum = 0\n",
    "  for index in range(k):\n",
    "    #need to add another + 1 because python lists starts from 0\n",
    "    sum += documents_relevance[index] / np.log2(index + 1 + 1)\n",
    "\n",
    "  return sum\n",
    "\n",
    "def calculate_ndcg(results, k):\n",
    "  ndcg_list = []\n",
    "  for key, items in results.items():\n",
    "    query = queries[queries['_id'] == key]['text'].iloc[0]\n",
    "    true_relevance = [calculate_true_relevance(query, corpus[corpus['_id'] == str(i)]['text'].iloc[0]) for i in items]\n",
    "    \n",
    "    dcg = calculate_dcg(true_relevance, k)\n",
    "    idcg = calculate_dcg(sorted(true_relevance, reverse=True), k)\n",
    "\n",
    "    ndcg_list.append(0 if dcg == 0 else dcg / idcg)\n",
    "  return np.mean(ndcg_list)\n",
    "\n",
    "def calculate_true_relevance(query, answer):\n",
    "  features = tokenizer(query + separator + answer,  padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    " \n",
    "  with torch.no_grad():\n",
    "      scores = model(**features).logits\n",
    "      return (scores[0][1] - scores[0][0]).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â€Zwroty wydatkÃ³w sÅ‚uÅ¼bowych na ogÃ³Å‚ nie podlegajÄ… opodatkowaniu, ale dojazdy z domu do pracy i z powrotem nie sÄ… uwaÅ¼ane za podrÃ³Å¼e sÅ‚uÅ¼bowe, a jeÅ›li za to pÅ‚acÄ…, jest to dochÃ³d podlegajÄ…cy opodatkowaniu. Nie sÄ…dzÄ™, aby carpooling to zmieniÅ‚, ale jestem nie jest prawnikiem podatkowym ani ksiÄ™gowym. PozostaÅ‚e pytania wydajÄ… siÄ™ dotyczyÄ‡ polityki firmy. Nie ma tu Å¼adnego â€powinienâ€. Nie musisz odbieraÄ‡ innych facetÃ³w, ale nie ma obowiÄ…zku ich zwrotu mile (lub zatrudniÄ‡), wiÄ™c zastanÃ³w siÄ™ dokÅ‚adnie nad swoimi priorytetami, zanim siÄ™ odepchniesz. Nigdy nie przywoÅ‚uj tego, czego nie moÅ¼esz odrzuciÄ‡â€.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[corpus['_id'] == '192843']['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 for answer_with_synonyms_with_lam: 0.8943496995792893\n",
      "NDCG@5 for answer_with_synonyms_without_lam: 0.8943496995792893\n",
      "NDCG@5 for answer_without_synonyms_with_lam: 1.0201039155423777\n",
      "NDCG@5 for answer_without_synonyms_without_lam: 1.0201039155423777\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(results_lam_syn)):\n",
    "  result = calculate_ndcg(results_lam_syn[indexes_types[j]], 5)\n",
    "  print(f\"NDCG@5 for {indexes_types[j]}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you think simpler methods, like Bayesian bag-of-words model, would work for sentence-pair classification? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that a Bayesian bag-of-words could work. For instance the method can capture patterns in frequencies of words, meaning that word can exist in question and not in answer and the opposite. This way we can have some key words for our sentences. In additon the method would not be that costly as our model. Unfortunelty the method would fall short on understanidng the context of the sentences. In conculsion i think that the method would work especially on small dataset as ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What hyper-parameters you have selected for the training? What resources (papers, tutorial) you have consulted to select these hyper-parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Think about pros and cons of the neural-network models with respect to natural language processing. Provide at least 2 pros and 2 cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pros:\n",
    "  - Can understand the context and relatshionship within sentences\n",
    "  - High accuracy \n",
    "- Cons:\n",
    "  - Training such models require powerfull resources\n",
    "  - Neural networks are often considered \"black-box\" models, meaning their decision-making process is difficult to interpret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
